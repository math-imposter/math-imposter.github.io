---
layout: default
title: Proposition 5.1
permalink: /proposition-5-1
---

<h2>Proposition 5.1</h2>

<h3 id="extended-proof">
  Extended proof of Proposition 5.1 with \(m^*\)
  <a href="#extended-proof" class="anchor-link" aria-label="Link to Section: Extended proof of Proposition 5.1 with m*">
    <i class="fas fa-link"></i>
  </a>
</h3>

We define \(m^*\) to be the smallest value such that \(m^* P_{\text{all}}\) is a p-variable, meaning that
\begin{equation*}
\mathbb{P}\left\{ m^* P_{\text{all}} \le \alpha \right\} \le \alpha.
\end{equation*}
We can rearrange this condition to be
\begin{equation*}
\mathbb{P}\left\{ P_{\text{all}} \le \frac{\alpha}{m^*} \right\} \le \alpha;
\end{equation*}
if we define \(\epsilon = \alpha / m^*\), this becomes
\begin{equation*}
\mathbb{P}\left\{ P_{\text{all}} \le \epsilon \right\} \le m^* \epsilon,
\end{equation*}
or, equivalently,
\begin{equation*}
\mathbb{P}\left\{ P_{\text{all}} > \epsilon \right\} \ge 1 - m^* \epsilon.
\end{equation*}

Let \(p_{\text{all}}(x, y)\) denote the function corresponding to \(P_{\text{all}}\). Then the prediction set
\begin{equation*}
C^{\text{data-avg}}_{\alpha}(X_{n+1}) = \left\{ y \in \mathcal{Y} : p_{\text{all}}(X_{n+1}, y) > \alpha \right\}
\end{equation*}
satisfies the coverage guarantee
\begin{equation*}
\mathbb{P} \left\{ Y_{n+1} \in C^{\text{data-avg}}_{\alpha}(X_{n+1}) \right\} \ge 1 - m^* \alpha.
\end{equation*}

Note that other transformations of \(P_{\text{all}}\) can also yield valid p-variables while preserving the relative weighting. For instance, if \(P_{\text{all}}\) is shifted rather than scaled, then the condition
\begin{equation*}
\mathbb{P}\left\{ m' + P_{\text{all}} \le \alpha \right\} \le \alpha
\end{equation*}
leads to a coverage guarantee of \(1 - (m' + \alpha)\). We focus on the scale correction factor \(m^*\) because it aligns with the framework of Vovk and Wang <a href="#ref-vovk2020">[1]</a> and works well in practice.

<h3 id="finite-sample" style="margin-top: 2em;">
  Finite-sample statement with \( \widehat{m}^* \)
  <a href="#finite-sample" class="anchor-link" aria-label="Link to Section: Finite-sample statement with m-hat star">
    <i class="fas fa-link"></i>
  </a>
</h3>

We define \(\widehat{m}^*\) to be the smallest value such that \(\widehat{m}^* \widehat{P}_{\text{all}}\) is a p-variable, where \(\widehat{P}_{\text{all}}\) is the empirical random variable associated with \(\widehat{F}^{\text{cons}}_{P_{\text{all}}}\).

Let \(A\) be the event that \(\widehat{m}^* \widehat{P}_{\text{all}} \leq \widehat{m}^* \alpha\) and \(B\) be the event that \(\widehat{m}^* \alpha < s\). Then, using the inequality \(\mathbb{P}(A) \leq \mathbb{P}(A \cap B) + \mathbb{P}(B^c)\), we have
\begin{equation} \label{eq:mhat_first_bound}
\mathbb{P}(\widehat{m}^* \widehat{P}_{\text{all}} \leq \widehat{m}^* \alpha) \leq \mathbb{P}(\widehat{m}^* \widehat{P}_{\text{all}} \leq s) + \mathbb{P}(\widehat{m}^* \alpha \geq s).
\end{equation}

By the definition of \(\widehat{m}^*\), the scaled variable \(\widehat{m}^* \widehat{P}_{\text{all}}\) is a p-variable, so it satisfies
\begin{equation*}
\mathbb{P}(\widehat{m}^* \widehat{P}_{\text{all}} \leq \alpha) \leq \alpha.
\end{equation*}

Markov's inequality tells us that
\begin{equation*}
\mathbb{P}(\widehat{m}^* \alpha \geq s) = \mathbb{P}\left( \widehat{m}^* \geq \frac{s}{\alpha} \right) \leq \frac{\mathbb{E}[\widehat{m}^*]}{s / \alpha} = \frac{\alpha \mathbb{E}[\widehat{m}^*]}{s}.
\end{equation*}

Combining these observations with \eqref{eq:mhat_first_bound}, we obtain
\begin{equation*}
\mathbb{P}(\widehat{P}_{\text{all}} \leq \alpha) \leq s + \frac{\alpha \mathbb{E}[\widehat{m}^*]}{s},
\end{equation*}
which holds for any \(s > 0\).

We optimize by minimizing the RHS over \(s\). The minimum of \(s + \frac{c}{s}\) occurs at \(s = \sqrt{c}\), so we choose \(s = \sqrt{\alpha \mathbb{E}[\widehat{m}^*]}\), which gives the bound
\begin{equation*}
\mathbb{P}(\widehat{P}_{\text{all}} \leq \alpha) \leq 2 \sqrt{ \alpha \mathbb{E}[\widehat{m}^*] },
\end{equation*}
or equivalently
\begin{equation*}
\mathbb{P}(\widehat{P}_{\text{all}} > \alpha) \geq 1 - 2 \sqrt{ \alpha \mathbb{E}[\widehat{m}^*] }.
\end{equation*}

Let \(\widehat{p}_{\text{all}}(x, y)\) denote the function corresponding to \(\widehat{P}_{\text{all}}\). Then the prediction set
\begin{equation*}
\widehat{C}^{\text{data-avg}}_{\alpha}(X_{n+1}) = \left\{ y \in \mathcal{Y} : \widehat{p}_{\text{all}}(X_{n+1}, y) > \alpha \right\}
\end{equation*}
satisfies the coverage guarantee
\begin{equation*}
\mathbb{P} \left\{ Y_{n+1} \in \widehat{C}^{\text{data-avg}}_{\alpha}(X_{n+1}) \right\} \ge 1 - 2 \sqrt{ \alpha \mathbb{E}[\widehat{m}^*] }.
\end{equation*}

<hr style="border: 1px solid #ddd; margin-top: 40px;">

<div id="ref-vovk2020">
  [1] Vovk and Wang (2020). <i>Combining p-values via averaging</i>.
</div>
